{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cover Letter Generator\n",
    "\n",
    "In today's competitive job market, submitting your resume often just isn't enough to make you stand out. Many companies now require a cover letter, but when you are applying to several jobs at once, writing an individual cover letter for each position can be time consuming. This project will solve this problem by creating a cover letter generator. \n",
    "\n",
    "The original goal was to use user information about the job listing and their experience extracted from a resume to create a unique cover letter for each job. Due to lack of training data, this generator can only generate generic cover letters with tokens that the user can fill in themselves.\n",
    "\n",
    "Further work on this project is needed to accomplish the original goal, specifically more training data (sample cover letters) are needed. Despite not accomplishing the original goal, this model could still be useful by giving the user a starting point when writing cover letters if more training data was obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow\n",
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I will define a function to checkout some of the corpus statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_stats(corpus):\n",
    "  print('Corpus Stats:')\n",
    "  print('Number of Documents: ' + str(len(corpus.fileids())))\n",
    "  print('Number of Paragraphs ' + str(len(corpus.paras())))\n",
    "  print('Number of sentences: ' + str(len(corpus.sents())))\n",
    "  print('Number of words: ' + str(len(corpus.words())))\n",
    "  print(\"Vocabulary: \" + str(len(set(w.lower() for w in corpus.words()))))\n",
    "  print(\"Avg chars per word: \" + str(round(len(corpus.raw())/len(corpus.words()),1)))\n",
    "  print(\"Avg words per sentence: \" + str(round(len(corpus.words())/len(corpus.sents()),1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the sample cover letter .txt files via NLTK's PlaintextCorpusReader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './cover_letter_samples'\n",
    "doc_pattern = r'.*\\.txt'\n",
    "corpus = PlaintextCorpusReader(path, doc_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus Stats:\n",
      "Number of Documents: 51\n",
      "Number of Paragraphs 246\n",
      "Number of sentences: 625\n",
      "Number of words: 14942\n",
      "Vocabulary: 2564\n",
      "Avg chars per word: 5.6\n",
      "Avg words per sentence: 23.9\n"
     ]
    }
   ],
   "source": [
    "corpus_stats(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [corpus.raw(fileid) for fileid in corpus.fileids()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove unwanted characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [doc.replace('\\n', '') for doc in docs]\n",
    "docs = [doc.replace('\\r', '') for doc in docs]\n",
    "docs = [doc.replace(')', '') for doc in docs]\n",
    "docs = [doc.replace('(', '') for doc in docs]\n",
    "docs = [doc.replace(',', '') for doc in docs]\n",
    "docs = [doc.replace('.', '') for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [doc.lower() for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of individual words for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [word_tokenize(doc) for doc in docs]\n",
    "\n",
    "lens = [len(token) for token in tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a single list of all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list = []\n",
    "for token in tokenized:\n",
    "    tokens_list.extend(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will encode the text to numeric vectors using BERT encoder because it is pre-trained and can understand the meaning of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tz = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tz.encode_plus(\n",
    "    text=tokens_list,  # the text to be encoded\n",
    "    add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "    max_length = len(tokens_list),  # maximum length of a document\n",
    "    truncation = True,\n",
    "    padding = 'max_length',  # Add [PAD]s\n",
    "    return_attention_mask = True,  # Generate the attention mask\n",
    "    return_tensors = 'tf',  # ask the function to return TensorFlow tensors\n",
    ")\n",
    "input_ids = encoded['input_ids']\n",
    "attn_mask = encoded['attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BERT encoder outputs a list of lists, so I will consolidate them into a single list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_list = []\n",
    "for input_id in input_ids:\n",
    "    input_ids_list.extend(input_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BERT encoder also outputs tensors, I need to convert them to numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_int = []\n",
    "\n",
    "for tensor in input_ids_list:\n",
    "    input_ids_int.append(tensor.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the words are numeric vectors, I will need to be able to decode the model's output. I will also need to be able to encode a test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word = {input_ids_int[i]: tokens_list[i] for i in range(len(input_ids_int))}\n",
    "word_to_id = {tokens_list[i]: input_ids_int[i] for i in range(len(tokens_list))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are modeling sequence-to-sequence, I will create sequences and the word that immediately follows that sequence to use as \"labels.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for i in range(0, len(input_ids_list) - seq_len, 3):\n",
    "    in_seq = input_ids_list[i:i+seq_len]\n",
    "    out_seq = input_ids_list[i + seq_len]\n",
    "    X.append(in_seq)\n",
    "    y.append(out_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape the input to (*time steps*, *batch size*, *something else*) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_array = np.reshape(X, (len(X), seq_len, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_array = np_utils.to_categorical(y).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27690, 4329)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X_array.shape[1], X_array.shape[2]), return_sequences=False))\n",
    "model.add(Dense(y_array.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm using ModelCheckpoint to save the model weights, so I don't have to retrain the model every time I restart the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "14/14 [==============================] - 4s 158ms/step - loss: 8.3515 - val_loss: 2.0955\n",
      "Epoch 2/150\n",
      "14/14 [==============================] - 1s 93ms/step - loss: 7.4857 - val_loss: 2.2609\n",
      "Epoch 3/150\n",
      "14/14 [==============================] - 1s 93ms/step - loss: 6.7753 - val_loss: 2.4251\n",
      "Epoch 4/150\n",
      "14/14 [==============================] - 1s 93ms/step - loss: 8.0722 - val_loss: 2.5683\n",
      "Epoch 5/150\n",
      "14/14 [==============================] - 1s 95ms/step - loss: 7.6351 - val_loss: 2.6990\n",
      "Epoch 6/150\n",
      "14/14 [==============================] - 1s 99ms/step - loss: 6.3028 - val_loss: 2.7912\n",
      "Epoch 7/150\n",
      "14/14 [==============================] - 1s 100ms/step - loss: 6.5035 - val_loss: 2.8342\n",
      "Epoch 8/150\n",
      "14/14 [==============================] - 1s 96ms/step - loss: 8.1242 - val_loss: 2.9107\n",
      "Epoch 9/150\n",
      "14/14 [==============================] - 1s 96ms/step - loss: 8.5028 - val_loss: 2.9825\n",
      "Epoch 10/150\n",
      "14/14 [==============================] - 1s 102ms/step - loss: 7.9307 - val_loss: 3.0336\n",
      "Epoch 11/150\n",
      "14/14 [==============================] - 1s 100ms/step - loss: 8.4596 - val_loss: 3.0760\n",
      "Epoch 12/150\n",
      "14/14 [==============================] - 1s 102ms/step - loss: 7.6002 - val_loss: 3.1107\n",
      "Epoch 13/150\n",
      "14/14 [==============================] - 1s 103ms/step - loss: 6.9691 - val_loss: 3.1636\n",
      "Epoch 14/150\n",
      "14/14 [==============================] - 1s 101ms/step - loss: 7.3542 - val_loss: 3.2020\n",
      "Epoch 15/150\n",
      "14/14 [==============================] - 2s 114ms/step - loss: 7.1407 - val_loss: 3.2495\n",
      "Epoch 16/150\n",
      "14/14 [==============================] - 1s 103ms/step - loss: 7.7340 - val_loss: 3.2695\n",
      "Epoch 17/150\n",
      "14/14 [==============================] - 1s 106ms/step - loss: 6.0395 - val_loss: 3.2732\n",
      "Epoch 18/150\n",
      "14/14 [==============================] - 1s 107ms/step - loss: 5.9867 - val_loss: 3.2994\n",
      "Epoch 19/150\n",
      "14/14 [==============================] - 1s 106ms/step - loss: 5.9891 - val_loss: 3.3959\n",
      "Epoch 20/150\n",
      "14/14 [==============================] - 1s 103ms/step - loss: 8.2103 - val_loss: 3.3664\n",
      "Epoch 21/150\n",
      "14/14 [==============================] - 1s 107ms/step - loss: 7.3947 - val_loss: 3.3783\n",
      "Epoch 22/150\n",
      "14/14 [==============================] - 1s 108ms/step - loss: 6.9453 - val_loss: 3.4115\n",
      "Epoch 23/150\n",
      "14/14 [==============================] - 1s 108ms/step - loss: 5.1718 - val_loss: 3.4706\n",
      "Epoch 24/150\n",
      "14/14 [==============================] - 1s 107ms/step - loss: 8.3751 - val_loss: 3.4754\n",
      "Epoch 25/150\n",
      "14/14 [==============================] - 1s 108ms/step - loss: 5.6988 - val_loss: 3.5221\n",
      "Epoch 26/150\n",
      "14/14 [==============================] - 2s 111ms/step - loss: 7.7848 - val_loss: 3.5076\n",
      "Epoch 27/150\n",
      "14/14 [==============================] - 2s 112ms/step - loss: 6.1280 - val_loss: 3.5471\n",
      "Epoch 28/150\n",
      "14/14 [==============================] - 2s 112ms/step - loss: 7.3573 - val_loss: 3.5600\n",
      "Epoch 29/150\n",
      "14/14 [==============================] - 2s 112ms/step - loss: 8.3381 - val_loss: 3.5897\n",
      "Epoch 30/150\n",
      "14/14 [==============================] - 2s 110ms/step - loss: 6.8437 - val_loss: 3.6477\n",
      "Epoch 31/150\n",
      "14/14 [==============================] - 1s 107ms/step - loss: 5.7442 - val_loss: 3.7099\n",
      "Epoch 32/150\n",
      "14/14 [==============================] - 2s 109ms/step - loss: 5.7921 - val_loss: 3.6965\n",
      "Epoch 33/150\n",
      "14/14 [==============================] - 2s 111ms/step - loss: 6.3844 - val_loss: 3.6878\n",
      "Epoch 34/150\n",
      "14/14 [==============================] - 2s 111ms/step - loss: 5.3058 - val_loss: 3.7582\n",
      "Epoch 35/150\n",
      "14/14 [==============================] - 2s 111ms/step - loss: 7.9278 - val_loss: 3.7663\n",
      "Epoch 36/150\n",
      "14/14 [==============================] - 2s 112ms/step - loss: 5.9028 - val_loss: 3.8287\n",
      "Epoch 37/150\n",
      "14/14 [==============================] - 2s 111ms/step - loss: 7.5728 - val_loss: 3.8330\n",
      "Epoch 38/150\n",
      "14/14 [==============================] - 2s 116ms/step - loss: 6.2273 - val_loss: 3.7599\n",
      "Epoch 39/150\n",
      "14/14 [==============================] - 2s 112ms/step - loss: 7.8287 - val_loss: 3.7907\n",
      "Epoch 40/150\n",
      "14/14 [==============================] - 2s 136ms/step - loss: 7.9268 - val_loss: 3.7717\n",
      "Epoch 41/150\n",
      "14/14 [==============================] - 2s 129ms/step - loss: 8.5533 - val_loss: 3.8229\n",
      "Epoch 42/150\n",
      "14/14 [==============================] - 2s 125ms/step - loss: 6.2376 - val_loss: 3.8646\n",
      "Epoch 43/150\n",
      "14/14 [==============================] - 2s 139ms/step - loss: 5.5536 - val_loss: 3.9305\n",
      "Epoch 44/150\n",
      "14/14 [==============================] - 2s 122ms/step - loss: 6.3034 - val_loss: 3.9492\n",
      "Epoch 45/150\n",
      "14/14 [==============================] - 2s 123ms/step - loss: 7.3105 - val_loss: 3.9063\n",
      "Epoch 46/150\n",
      "14/14 [==============================] - 2s 124ms/step - loss: 7.9332 - val_loss: 4.0039\n",
      "Epoch 47/150\n",
      "14/14 [==============================] - 2s 129ms/step - loss: 6.1855 - val_loss: 4.0213\n",
      "Epoch 48/150\n",
      "14/14 [==============================] - 2s 122ms/step - loss: 6.0368 - val_loss: 4.0516\n",
      "Epoch 49/150\n",
      "14/14 [==============================] - 2s 126ms/step - loss: 5.4879 - val_loss: 4.0839\n",
      "Epoch 50/150\n",
      "14/14 [==============================] - 2s 137ms/step - loss: 6.8928 - val_loss: 4.0988\n",
      "Epoch 51/150\n",
      "14/14 [==============================] - 2s 154ms/step - loss: 4.2843 - val_loss: 4.1875\n",
      "Epoch 52/150\n",
      "14/14 [==============================] - 2s 164ms/step - loss: 6.7008 - val_loss: 4.0578\n",
      "Epoch 53/150\n",
      "14/14 [==============================] - 2s 158ms/step - loss: 5.3771 - val_loss: 4.0417\n",
      "Epoch 54/150\n",
      "14/14 [==============================] - 2s 110ms/step - loss: 6.6221 - val_loss: 4.0502\n",
      "Epoch 55/150\n",
      "14/14 [==============================] - 2s 128ms/step - loss: 6.1544 - val_loss: 4.1293\n",
      "Epoch 56/150\n",
      "14/14 [==============================] - 2s 151ms/step - loss: 5.6924 - val_loss: 4.1702\n",
      "Epoch 57/150\n",
      "14/14 [==============================] - 2s 149ms/step - loss: 5.5069 - val_loss: 4.1509\n",
      "Epoch 58/150\n",
      "14/14 [==============================] - 2s 133ms/step - loss: 5.7619 - val_loss: 4.2130\n",
      "Epoch 59/150\n",
      "14/14 [==============================] - 2s 145ms/step - loss: 8.0974 - val_loss: 4.2074\n",
      "Epoch 60/150\n",
      "14/14 [==============================] - 2s 148ms/step - loss: 6.2029 - val_loss: 4.3151\n",
      "Epoch 61/150\n",
      "14/14 [==============================] - 2s 163ms/step - loss: 5.5300 - val_loss: 4.3356\n",
      "Epoch 62/150\n",
      "14/14 [==============================] - 2s 141ms/step - loss: 8.3221 - val_loss: 4.3262\n",
      "Epoch 63/150\n",
      "14/14 [==============================] - 2s 147ms/step - loss: 5.5372 - val_loss: 4.4452\n",
      "Epoch 64/150\n",
      "14/14 [==============================] - 2s 166ms/step - loss: 5.8153 - val_loss: 4.3939\n",
      "Epoch 65/150\n",
      "14/14 [==============================] - 2s 154ms/step - loss: 5.0476 - val_loss: 4.4671\n",
      "Epoch 66/150\n",
      "14/14 [==============================] - 2s 135ms/step - loss: 6.1231 - val_loss: 4.4842\n",
      "Epoch 67/150\n",
      "14/14 [==============================] - 2s 139ms/step - loss: 5.3004 - val_loss: 4.6541\n",
      "Epoch 68/150\n",
      "14/14 [==============================] - 2s 140ms/step - loss: 5.1378 - val_loss: 4.6083\n",
      "Epoch 69/150\n",
      "14/14 [==============================] - 2s 145ms/step - loss: 5.6572 - val_loss: 4.6123\n",
      "Epoch 70/150\n",
      "14/14 [==============================] - 2s 127ms/step - loss: 8.0117 - val_loss: 4.6356\n",
      "Epoch 71/150\n",
      "14/14 [==============================] - 2s 141ms/step - loss: 5.7595 - val_loss: 4.7065\n",
      "Epoch 72/150\n",
      "14/14 [==============================] - 2s 142ms/step - loss: 6.8544 - val_loss: 4.5381\n",
      "Epoch 73/150\n",
      "14/14 [==============================] - 2s 115ms/step - loss: 7.4726 - val_loss: 4.4479\n",
      "Epoch 74/150\n",
      "14/14 [==============================] - 2s 117ms/step - loss: 5.4360 - val_loss: 4.4454\n",
      "Epoch 75/150\n",
      "14/14 [==============================] - 2s 118ms/step - loss: 5.2665 - val_loss: 4.4594\n",
      "Epoch 76/150\n",
      "14/14 [==============================] - 2s 116ms/step - loss: 4.9270 - val_loss: 4.6121\n",
      "Epoch 77/150\n",
      "14/14 [==============================] - 2s 115ms/step - loss: 5.9257 - val_loss: 4.4379\n",
      "Epoch 78/150\n",
      "14/14 [==============================] - 2s 114ms/step - loss: 6.1204 - val_loss: 4.4539\n",
      "Epoch 79/150\n",
      "14/14 [==============================] - 2s 114ms/step - loss: 5.1543 - val_loss: 4.4613\n",
      "Epoch 80/150\n",
      "14/14 [==============================] - 2s 115ms/step - loss: 8.6679 - val_loss: 4.3933\n",
      "Epoch 81/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 2s 114ms/step - loss: 5.6145 - val_loss: 4.3734\n",
      "Epoch 82/150\n",
      "14/14 [==============================] - 2s 117ms/step - loss: 5.5530 - val_loss: 4.3836\n",
      "Epoch 83/150\n",
      "14/14 [==============================] - 2s 112ms/step - loss: 8.2115 - val_loss: 4.3690\n",
      "Epoch 84/150\n",
      "14/14 [==============================] - 2s 115ms/step - loss: 6.3732 - val_loss: 4.4820\n",
      "Epoch 85/150\n",
      "14/14 [==============================] - 2s 113ms/step - loss: 7.5149 - val_loss: 4.3873\n",
      "Epoch 86/150\n",
      "14/14 [==============================] - 2s 118ms/step - loss: 7.1301 - val_loss: 4.4040\n",
      "Epoch 87/150\n",
      "14/14 [==============================] - 2s 114ms/step - loss: 5.7600 - val_loss: 4.5054\n",
      "Epoch 88/150\n",
      "14/14 [==============================] - 2s 116ms/step - loss: 5.7254 - val_loss: 4.5100\n",
      "Epoch 89/150\n",
      "14/14 [==============================] - 2s 117ms/step - loss: 5.5202 - val_loss: 4.6208\n",
      "Epoch 90/150\n",
      "14/14 [==============================] - 2s 116ms/step - loss: 7.2549 - val_loss: 4.5917\n",
      "Epoch 91/150\n",
      "14/14 [==============================] - 2s 117ms/step - loss: 6.5387 - val_loss: 4.7240\n",
      "Epoch 92/150\n",
      "14/14 [==============================] - 2s 117ms/step - loss: 7.8175 - val_loss: 4.6963\n",
      "Epoch 93/150\n",
      "14/14 [==============================] - 2s 118ms/step - loss: 6.3837 - val_loss: 4.8262\n",
      "Epoch 94/150\n",
      "14/14 [==============================] - 2s 115ms/step - loss: 4.6836 - val_loss: 4.7934\n",
      "Epoch 95/150\n",
      "14/14 [==============================] - 2s 125ms/step - loss: 6.2912 - val_loss: 4.9894\n",
      "Epoch 96/150\n",
      "14/14 [==============================] - 2s 119ms/step - loss: 5.6234 - val_loss: 4.9675\n",
      "Epoch 97/150\n",
      "14/14 [==============================] - 2s 122ms/step - loss: 5.3016 - val_loss: 5.0370\n",
      "Epoch 98/150\n",
      "14/14 [==============================] - 2s 117ms/step - loss: 5.4868 - val_loss: 5.1954\n",
      "Epoch 99/150\n",
      "14/14 [==============================] - 2s 113ms/step - loss: 7.0048 - val_loss: 5.0598\n",
      "Epoch 100/150\n",
      "14/14 [==============================] - 2s 115ms/step - loss: 5.6730 - val_loss: 5.1515\n",
      "Epoch 101/150\n",
      "14/14 [==============================] - 2s 113ms/step - loss: 7.3615 - val_loss: 5.0661\n",
      "Epoch 102/150\n",
      "14/14 [==============================] - 2s 114ms/step - loss: 6.1164 - val_loss: 5.0969\n",
      "Epoch 103/150\n",
      "14/14 [==============================] - 2s 113ms/step - loss: 6.2806 - val_loss: 5.1381\n",
      "Epoch 104/150\n",
      "14/14 [==============================] - 2s 112ms/step - loss: 6.3499 - val_loss: 5.1820\n",
      "Epoch 105/150\n",
      "14/14 [==============================] - 2s 113ms/step - loss: 5.1139 - val_loss: 5.2306\n",
      "Epoch 106/150\n",
      "14/14 [==============================] - 2s 114ms/step - loss: 6.0864 - val_loss: 5.2168\n",
      "Epoch 107/150\n",
      "14/14 [==============================] - 2s 112ms/step - loss: 6.9290 - val_loss: 5.2727\n",
      "Epoch 108/150\n",
      "14/14 [==============================] - 2s 111ms/step - loss: 4.5727 - val_loss: 5.2561\n",
      "Epoch 109/150\n",
      "14/14 [==============================] - 2s 111ms/step - loss: 5.3401 - val_loss: 5.3415\n",
      "Epoch 110/150\n",
      "14/14 [==============================] - 2s 115ms/step - loss: 5.2301 - val_loss: 5.5715\n",
      "Epoch 111/150\n",
      "14/14 [==============================] - 2s 113ms/step - loss: 6.3970 - val_loss: 5.3971\n",
      "Epoch 112/150\n",
      "14/14 [==============================] - 2s 112ms/step - loss: 6.8462 - val_loss: 5.6657\n",
      "Epoch 113/150\n",
      "14/14 [==============================] - 2s 116ms/step - loss: 7.4879 - val_loss: 5.5798\n",
      "Epoch 114/150\n",
      "14/14 [==============================] - 2s 116ms/step - loss: 6.1957 - val_loss: 5.6752\n",
      "Epoch 115/150\n",
      "14/14 [==============================] - 2s 117ms/step - loss: 4.8668 - val_loss: 5.6151\n",
      "Epoch 116/150\n",
      "14/14 [==============================] - 2s 143ms/step - loss: 5.5189 - val_loss: 5.7001\n",
      "Epoch 117/150\n",
      "14/14 [==============================] - 2s 127ms/step - loss: 5.6394 - val_loss: 5.7042\n",
      "Epoch 118/150\n",
      "14/14 [==============================] - 2s 148ms/step - loss: 6.1929 - val_loss: 5.7156\n",
      "Epoch 119/150\n",
      "14/14 [==============================] - 2s 117ms/step - loss: 5.2046 - val_loss: 5.7587\n",
      "Epoch 120/150\n",
      "14/14 [==============================] - 2s 123ms/step - loss: 6.0039 - val_loss: 5.7468\n",
      "Epoch 121/150\n",
      "14/14 [==============================] - 2s 115ms/step - loss: 6.4820 - val_loss: 5.7948\n",
      "Epoch 122/150\n",
      "14/14 [==============================] - 2s 123ms/step - loss: 4.4815 - val_loss: 5.9238\n",
      "Epoch 123/150\n",
      "14/14 [==============================] - 2s 120ms/step - loss: 5.3673 - val_loss: 5.8476\n",
      "Epoch 124/150\n",
      "14/14 [==============================] - 2s 116ms/step - loss: 7.6722 - val_loss: 5.8767\n",
      "Epoch 125/150\n",
      "14/14 [==============================] - 2s 116ms/step - loss: 5.5217 - val_loss: 5.7843\n",
      "Epoch 126/150\n",
      "14/14 [==============================] - 2s 116ms/step - loss: 6.5079 - val_loss: 6.1169\n",
      "Epoch 127/150\n",
      "14/14 [==============================] - 2s 117ms/step - loss: 6.2991 - val_loss: 6.0040\n",
      "Epoch 128/150\n",
      "14/14 [==============================] - 2s 141ms/step - loss: 5.3418 - val_loss: 5.9588\n",
      "Epoch 129/150\n",
      "14/14 [==============================] - 2s 119ms/step - loss: 5.5664 - val_loss: 5.7809\n",
      "Epoch 130/150\n",
      "14/14 [==============================] - 2s 116ms/step - loss: 7.3113 - val_loss: 5.8620\n",
      "Epoch 131/150\n",
      "14/14 [==============================] - 2s 114ms/step - loss: 6.1913 - val_loss: 5.9033\n",
      "Epoch 132/150\n",
      "14/14 [==============================] - 2s 119ms/step - loss: 6.4376 - val_loss: 6.0049\n",
      "Epoch 133/150\n",
      "14/14 [==============================] - 2s 122ms/step - loss: 6.6538 - val_loss: 5.9094\n",
      "Epoch 134/150\n",
      "14/14 [==============================] - 2s 132ms/step - loss: 5.0593 - val_loss: 6.4086\n",
      "Epoch 135/150\n",
      "14/14 [==============================] - 2s 127ms/step - loss: 7.3533 - val_loss: 6.1266\n",
      "Epoch 136/150\n",
      "14/14 [==============================] - 2s 125ms/step - loss: 6.4257 - val_loss: 6.1465\n",
      "Epoch 137/150\n",
      "14/14 [==============================] - 2s 133ms/step - loss: 6.4592 - val_loss: 6.0551\n",
      "Epoch 138/150\n",
      "14/14 [==============================] - 1s 107ms/step - loss: 7.5093 - val_loss: 6.1586\n",
      "Epoch 139/150\n",
      "14/14 [==============================] - 2s 115ms/step - loss: 7.0452 - val_loss: 6.1200\n",
      "Epoch 140/150\n",
      "14/14 [==============================] - 2s 130ms/step - loss: 6.1190 - val_loss: 6.2381\n",
      "Epoch 141/150\n",
      "14/14 [==============================] - 2s 115ms/step - loss: 5.7641 - val_loss: 6.3024\n",
      "Epoch 142/150\n",
      "14/14 [==============================] - 2s 120ms/step - loss: 5.5321 - val_loss: 6.0726\n",
      "Epoch 143/150\n",
      "14/14 [==============================] - 2s 126ms/step - loss: 4.6928 - val_loss: 6.1460\n",
      "Epoch 144/150\n",
      "14/14 [==============================] - 2s 112ms/step - loss: 5.6268 - val_loss: 6.2149\n",
      "Epoch 145/150\n",
      "14/14 [==============================] - 2s 111ms/step - loss: 5.6819 - val_loss: 6.2312\n",
      "Epoch 146/150\n",
      "14/14 [==============================] - 2s 142ms/step - loss: 4.7668 - val_loss: 6.4606\n",
      "Epoch 147/150\n",
      "14/14 [==============================] - 2s 130ms/step - loss: 6.1613 - val_loss: 6.2913\n",
      "Epoch 148/150\n",
      "14/14 [==============================] - 2s 122ms/step - loss: 5.4033 - val_loss: 6.3429\n",
      "Epoch 149/150\n",
      "14/14 [==============================] - 2s 113ms/step - loss: 5.5197 - val_loss: 6.3743\n",
      "Epoch 150/150\n",
      "14/14 [==============================] - 2s 120ms/step - loss: 5.2566 - val_loss: 6.3846\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_array, y_array, validation_split = 0.2, epochs=150, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss - ADAM')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is underfit, evidenced by the high test loss that doesn't converge. More training data might solve this problem. Because of the underfitting, the model will likely not output good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to test the model. I input the beginning of a cover letter. The loop deletes the first token in the sample, predicts on a sequence of the specified length, append the predicted word to the end of the sample. The first word is dropped and the model predicts on the next sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would like to apply would countries from i where 9 completed experience the would countries from i where 9 completed experience the would countries from i where 9 completed\n"
     ]
    }
   ],
   "source": [
    "sample = \"I would like to apply\"\n",
    "sample = sample.lower()\n",
    "sample_split = word_tokenize(sample)\n",
    "sample_split.insert(0, 0)\n",
    "\n",
    "for i in range(25):\n",
    "    sample_split = sample_split[1:]\n",
    "    \n",
    "    sample_ids = [word_to_id[word] for word in sample_split]\n",
    "\n",
    "    sample_array = tf.convert_to_tensor(sample_ids)\n",
    "    sample_array = [list(sample_array)]\n",
    "    sample_array = np.reshape(sample_array, (1, seq_len, 1))\n",
    "    prediction = model.predict(sample_array)\n",
    "\n",
    "\n",
    "    pred_index = prediction[0].argmax()\n",
    "    new_word = tokens_list[pred_index]\n",
    "    sample_split.append(new_word)\n",
    "    sample = sample + ' ' + new_word\n",
    "\n",
    "sample = sample.capitalize()\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is the same as model 1 except the optimzer is sigmoid gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Sequential()\n",
    "model_2.add(LSTM(256, input_shape=(X_array.shape[1], X_array.shape[2])))\n",
    "model_2.add(Dense(y_array.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.compile(loss='categorical_crossentropy', optimizer='SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "14/14 [==============================] - 4s 144ms/step - loss: 7.5994 - val_loss: 1.9752\n",
      "Epoch 2/150\n",
      "14/14 [==============================] - 2s 114ms/step - loss: 9.2873 - val_loss: 1.9756\n",
      "Epoch 3/150\n",
      "14/14 [==============================] - 2s 115ms/step - loss: 6.7208 - val_loss: 1.9762\n",
      "Epoch 4/150\n",
      "14/14 [==============================] - 2s 115ms/step - loss: 7.2711 - val_loss: 1.9765\n",
      "Epoch 5/150\n",
      "14/14 [==============================] - 2s 112ms/step - loss: 9.4867 - val_loss: 1.9767\n",
      "Epoch 6/150\n",
      "14/14 [==============================] - 2s 113ms/step - loss: 8.2088 - val_loss: 1.9770\n",
      "Epoch 7/150\n",
      "14/14 [==============================] - 2s 110ms/step - loss: 6.6097 - val_loss: 1.9773\n",
      "Epoch 8/150\n",
      "14/14 [==============================] - 2s 117ms/step - loss: 7.4260 - val_loss: 1.9776\n",
      "Epoch 9/150\n",
      "14/14 [==============================] - 2s 111ms/step - loss: 8.4304 - val_loss: 1.9779\n",
      "Epoch 10/150\n",
      "14/14 [==============================] - 2s 114ms/step - loss: 12.5392 - val_loss: 1.9783\n",
      "Epoch 11/150\n",
      "14/14 [==============================] - 2s 109ms/step - loss: 8.0560 - val_loss: 1.9786\n",
      "Epoch 12/150\n",
      "14/14 [==============================] - 2s 122ms/step - loss: 9.7131 - val_loss: 1.9789\n",
      "Epoch 13/150\n",
      "14/14 [==============================] - 1s 108ms/step - loss: 9.1582 - val_loss: 1.9792\n",
      "Epoch 14/150\n",
      "14/14 [==============================] - 1s 108ms/step - loss: 8.3743 - val_loss: 1.9795\n",
      "Epoch 15/150\n",
      "14/14 [==============================] - 2s 112ms/step - loss: 11.4303 - val_loss: 1.9799\n",
      "Epoch 16/150\n",
      "14/14 [==============================] - 2s 110ms/step - loss: 7.5674 - val_loss: 1.9802\n",
      "Epoch 17/150\n",
      "14/14 [==============================] - 2s 110ms/step - loss: 8.6400 - val_loss: 1.9805\n",
      "Epoch 18/150\n",
      "14/14 [==============================] - 1s 107ms/step - loss: 7.8420 - val_loss: 1.9809\n",
      "Epoch 19/150\n",
      "14/14 [==============================] - 1s 108ms/step - loss: 6.7370 - val_loss: 1.9812\n",
      "Epoch 20/150\n",
      "14/14 [==============================] - 2s 109ms/step - loss: 8.9865 - val_loss: 1.9815\n",
      "Epoch 21/150\n",
      "14/14 [==============================] - 1s 108ms/step - loss: 7.5250 - val_loss: 1.9819\n",
      "Epoch 22/150\n",
      "14/14 [==============================] - 1s 108ms/step - loss: 8.2402 - val_loss: 1.9822\n",
      "Epoch 23/150\n",
      "14/14 [==============================] - 2s 113ms/step - loss: 9.9150 - val_loss: 1.9825\n",
      "Epoch 24/150\n",
      "14/14 [==============================] - 2s 110ms/step - loss: 8.2413 - val_loss: 1.9829\n",
      "Epoch 25/150\n",
      "14/14 [==============================] - 2s 110ms/step - loss: 7.3969 - val_loss: 1.9832\n",
      "Epoch 26/150\n",
      "14/14 [==============================] - 1s 108ms/step - loss: 10.0706 - val_loss: 1.9835\n",
      "Epoch 27/150\n",
      "14/14 [==============================] - 2s 110ms/step - loss: 7.2806 - val_loss: 1.9839\n",
      "Epoch 28/150\n",
      "14/14 [==============================] - 1s 108ms/step - loss: 8.1620 - val_loss: 1.9838\n",
      "Epoch 29/150\n",
      "14/14 [==============================] - 1s 108ms/step - loss: 7.5128 - val_loss: 1.9845\n",
      "Epoch 30/150\n",
      "14/14 [==============================] - 2s 113ms/step - loss: 7.7970 - val_loss: 1.9849\n",
      "Epoch 31/150\n",
      "14/14 [==============================] - 2s 112ms/step - loss: 7.0467 - val_loss: 1.9853\n",
      "Epoch 32/150\n",
      "14/14 [==============================] - 2s 109ms/step - loss: 7.9901 - val_loss: 1.9862\n",
      "Epoch 33/150\n",
      "14/14 [==============================] - 1s 108ms/step - loss: 6.9851 - val_loss: 1.9865\n",
      "Epoch 34/150\n",
      "14/14 [==============================] - 1s 107ms/step - loss: 6.6900 - val_loss: 1.9868\n",
      "Epoch 35/150\n",
      "14/14 [==============================] - 2s 109ms/step - loss: 7.0106 - val_loss: 1.9871\n",
      "Epoch 36/150\n",
      "14/14 [==============================] - 1s 108ms/step - loss: 6.9449 - val_loss: 1.9875\n",
      "Epoch 37/150\n",
      "14/14 [==============================] - 2s 110ms/step - loss: 8.9870 - val_loss: 1.9878\n",
      "Epoch 38/150\n",
      "14/14 [==============================] - 2s 112ms/step - loss: 8.8752 - val_loss: 1.9881\n",
      "Epoch 39/150\n",
      "14/14 [==============================] - 2s 113ms/step - loss: 9.2098 - val_loss: 1.9885\n",
      "Epoch 40/150\n",
      "14/14 [==============================] - 2s 115ms/step - loss: 6.7644 - val_loss: 1.9896\n",
      "Epoch 41/150\n",
      "14/14 [==============================] - 2s 112ms/step - loss: 8.2467 - val_loss: 1.9900\n",
      "Epoch 42/150\n",
      "14/14 [==============================] - 2s 123ms/step - loss: 10.1485 - val_loss: 1.9903\n",
      "Epoch 43/150\n",
      "14/14 [==============================] - 2s 111ms/step - loss: 9.6797 - val_loss: 1.9906\n",
      "Epoch 44/150\n",
      "14/14 [==============================] - 2s 114ms/step - loss: 7.7712 - val_loss: 1.9909\n",
      "Epoch 45/150\n",
      "14/14 [==============================] - 2s 114ms/step - loss: 8.7240 - val_loss: 1.9912\n",
      "Epoch 46/150\n",
      "14/14 [==============================] - 2s 118ms/step - loss: 7.7673 - val_loss: 1.9916\n",
      "Epoch 47/150\n",
      "14/14 [==============================] - 2s 116ms/step - loss: 7.2862 - val_loss: 1.9919\n",
      "Epoch 48/150\n",
      "14/14 [==============================] - 2s 114ms/step - loss: 8.7008 - val_loss: 1.9922\n",
      "Epoch 49/150\n",
      "14/14 [==============================] - 2s 120ms/step - loss: 9.9213 - val_loss: 1.9926\n",
      "Epoch 50/150\n",
      "14/14 [==============================] - 2s 117ms/step - loss: 9.1578 - val_loss: 1.9929\n",
      "Epoch 51/150\n",
      "14/14 [==============================] - 2s 110ms/step - loss: 8.4658 - val_loss: 1.9932\n",
      "Epoch 52/150\n",
      "14/14 [==============================] - 2s 110ms/step - loss: 7.4961 - val_loss: 1.9936\n",
      "Epoch 53/150\n",
      "14/14 [==============================] - 2s 112ms/step - loss: 7.9162 - val_loss: 1.9939\n",
      "Epoch 54/150\n",
      "14/14 [==============================] - 2s 110ms/step - loss: 10.3070 - val_loss: 1.9942\n",
      "Epoch 55/150\n",
      "14/14 [==============================] - 2s 109ms/step - loss: 8.0786 - val_loss: 1.9945\n",
      "Epoch 56/150\n",
      "14/14 [==============================] - 1s 108ms/step - loss: 7.4520 - val_loss: 1.9949\n",
      "Epoch 57/150\n",
      "14/14 [==============================] - 2s 108ms/step - loss: 8.9947 - val_loss: 1.9952\n",
      "Epoch 58/150\n",
      "14/14 [==============================] - 2s 110ms/step - loss: 11.9603 - val_loss: 1.9956\n",
      "Epoch 59/150\n",
      "14/14 [==============================] - 2s 109ms/step - loss: 11.1905 - val_loss: 1.9959\n",
      "Epoch 60/150\n",
      "14/14 [==============================] - 2s 113ms/step - loss: 8.1004 - val_loss: 1.9962\n",
      "Epoch 61/150\n",
      "14/14 [==============================] - 2s 109ms/step - loss: 8.4951 - val_loss: 1.9966\n",
      "Epoch 62/150\n",
      "14/14 [==============================] - 2s 111ms/step - loss: 8.4674 - val_loss: 1.9969\n",
      "Epoch 63/150\n",
      "14/14 [==============================] - 2s 109ms/step - loss: 6.4505 - val_loss: 1.9972\n",
      "Epoch 64/150\n",
      "14/14 [==============================] - 2s 109ms/step - loss: 6.5092 - val_loss: 1.9976\n",
      "Epoch 65/150\n",
      "14/14 [==============================] - 2s 114ms/step - loss: 6.8133 - val_loss: 1.9979\n",
      "Epoch 66/150\n",
      "14/14 [==============================] - 2s 110ms/step - loss: 7.1691 - val_loss: 1.9982\n",
      "Epoch 67/150\n",
      "14/14 [==============================] - 2s 120ms/step - loss: 7.7022 - val_loss: 1.9985\n",
      "Epoch 68/150\n",
      "14/14 [==============================] - 2s 112ms/step - loss: 8.0705 - val_loss: 1.9989\n",
      "Epoch 69/150\n",
      "14/14 [==============================] - 1s 108ms/step - loss: 6.4525 - val_loss: 1.9992\n",
      "Epoch 70/150\n",
      "14/14 [==============================] - 1s 107ms/step - loss: 8.9990 - val_loss: 1.9995\n",
      "Epoch 71/150\n",
      "14/14 [==============================] - 2s 110ms/step - loss: 9.7359 - val_loss: 1.9999\n",
      "Epoch 72/150\n",
      "14/14 [==============================] - 2s 109ms/step - loss: 9.4901 - val_loss: 2.0002\n",
      "Epoch 73/150\n",
      "14/14 [==============================] - 2s 111ms/step - loss: 8.4122 - val_loss: 2.0006\n",
      "Epoch 74/150\n",
      "14/14 [==============================] - 2s 114ms/step - loss: 8.9713 - val_loss: 2.0009\n",
      "Epoch 75/150\n",
      "14/14 [==============================] - 2s 114ms/step - loss: 8.7586 - val_loss: 2.0013\n",
      "Epoch 76/150\n",
      "14/14 [==============================] - 2s 109ms/step - loss: 8.1577 - val_loss: 2.0017\n",
      "Epoch 77/150\n",
      "14/14 [==============================] - 2s 109ms/step - loss: 9.5767 - val_loss: 2.0020\n",
      "Epoch 78/150\n",
      "14/14 [==============================] - 1s 107ms/step - loss: 7.8947 - val_loss: 2.0023\n",
      "Epoch 79/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 6.953 - 1s 107ms/step - loss: 7.0477 - val_loss: 2.0027\n",
      "Epoch 80/150\n",
      "14/14 [==============================] - 1s 108ms/step - loss: 9.7034 - val_loss: 2.0030\n",
      "Epoch 81/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - ETA: 0s - loss: 7.965 - 2s 112ms/step - loss: 7.9918 - val_loss: 2.0033\n",
      "Epoch 82/150\n",
      "14/14 [==============================] - 2s 111ms/step - loss: 8.7273 - val_loss: 2.0036\n",
      "Epoch 83/150\n",
      "14/14 [==============================] - 1s 108ms/step - loss: 6.8384 - val_loss: 2.0040\n",
      "Epoch 84/150\n",
      "14/14 [==============================] - 1s 108ms/step - loss: 10.4531 - val_loss: 2.0043\n",
      "Epoch 85/150\n",
      "14/14 [==============================] - 1s 108ms/step - loss: 8.3330 - val_loss: 2.0047\n",
      "Epoch 86/150\n",
      "14/14 [==============================] - 1s 108ms/step - loss: 8.1347 - val_loss: 2.0051\n",
      "Epoch 87/150\n",
      "14/14 [==============================] - 2s 110ms/step - loss: 6.7960 - val_loss: 2.0054\n",
      "Epoch 88/150\n",
      "14/14 [==============================] - 2s 109ms/step - loss: 8.2062 - val_loss: 2.0057\n",
      "Epoch 89/150\n",
      "14/14 [==============================] - 1s 108ms/step - loss: 7.1955 - val_loss: 2.0060\n",
      "Epoch 90/150\n",
      "14/14 [==============================] - 2s 108ms/step - loss: 9.3306 - val_loss: 2.0063\n",
      "Epoch 91/150\n",
      "14/14 [==============================] - 2s 111ms/step - loss: 7.0258 - val_loss: 2.0067\n",
      "Epoch 92/150\n",
      "14/14 [==============================] - 2s 109ms/step - loss: 7.5765 - val_loss: 2.0070\n",
      "Epoch 93/150\n",
      "14/14 [==============================] - 2s 109ms/step - loss: 6.3828 - val_loss: 2.0073\n",
      "Epoch 94/150\n",
      "14/14 [==============================] - 1s 107ms/step - loss: 7.7948 - val_loss: 2.0076\n",
      "Epoch 95/150\n",
      "14/14 [==============================] - 2s 110ms/step - loss: 8.4686 - val_loss: 2.0080\n",
      "Epoch 96/150\n",
      "14/14 [==============================] - 2s 116ms/step - loss: 8.5365 - val_loss: 2.0083\n",
      "Epoch 97/150\n",
      "14/14 [==============================] - 1s 106ms/step - loss: 6.8045 - val_loss: 2.0086\n",
      "Epoch 98/150\n",
      "14/14 [==============================] - 2s 109ms/step - loss: 8.5394 - val_loss: 2.0089\n",
      "Epoch 99/150\n",
      "14/14 [==============================] - 2s 136ms/step - loss: 8.4285 - val_loss: 2.0092\n",
      "Epoch 100/150\n",
      "14/14 [==============================] - 2s 123ms/step - loss: 8.1784 - val_loss: 2.0095\n",
      "Epoch 101/150\n",
      "14/14 [==============================] - 2s 131ms/step - loss: 9.0786 - val_loss: 2.0093\n",
      "Epoch 102/150\n",
      "14/14 [==============================] - 2s 127ms/step - loss: 9.3629 - val_loss: 2.0096\n",
      "Epoch 103/150\n",
      "14/14 [==============================] - 2s 139ms/step - loss: 7.8089 - val_loss: 2.0099\n",
      "Epoch 104/150\n",
      "14/14 [==============================] - 2s 152ms/step - loss: 6.3267 - val_loss: 2.0102\n",
      "Epoch 105/150\n",
      "14/14 [==============================] - 2s 121ms/step - loss: 8.3755 - val_loss: 2.0105\n",
      "Epoch 106/150\n",
      "14/14 [==============================] - 2s 135ms/step - loss: 7.9696 - val_loss: 2.0108\n",
      "Epoch 107/150\n",
      "14/14 [==============================] - 2s 142ms/step - loss: 10.2152 - val_loss: 2.0112\n",
      "Epoch 108/150\n",
      "14/14 [==============================] - 2s 110ms/step - loss: 7.8380 - val_loss: 2.0116\n",
      "Epoch 109/150\n",
      "14/14 [==============================] - 2s 113ms/step - loss: 6.4487 - val_loss: 2.0123\n",
      "Epoch 110/150\n",
      "14/14 [==============================] - 2s 109ms/step - loss: 9.1894 - val_loss: 2.0126\n",
      "Epoch 111/150\n",
      "14/14 [==============================] - 1s 107ms/step - loss: 11.3055 - val_loss: 2.0130\n",
      "Epoch 112/150\n",
      "14/14 [==============================] - 2s 109ms/step - loss: 7.2303 - val_loss: 2.0133\n",
      "Epoch 113/150\n",
      "14/14 [==============================] - 2s 109ms/step - loss: 7.9413 - val_loss: 2.0137\n",
      "Epoch 114/150\n",
      "14/14 [==============================] - 2s 111ms/step - loss: 6.8146 - val_loss: 2.0140\n",
      "Epoch 115/150\n",
      "14/14 [==============================] - 2s 114ms/step - loss: 9.9624 - val_loss: 2.0143\n",
      "Epoch 116/150\n",
      "14/14 [==============================] - 2s 110ms/step - loss: 9.2094 - val_loss: 2.0147\n",
      "Epoch 117/150\n",
      "14/14 [==============================] - 2s 109ms/step - loss: 8.1888 - val_loss: 2.0149\n",
      "Epoch 118/150\n",
      "14/14 [==============================] - 1s 107ms/step - loss: 8.8875 - val_loss: 2.0153\n",
      "Epoch 119/150\n",
      "14/14 [==============================] - 2s 112ms/step - loss: 9.3272 - val_loss: 2.0156\n",
      "Epoch 120/150\n",
      "14/14 [==============================] - 2s 118ms/step - loss: 7.9880 - val_loss: 2.0159\n",
      "Epoch 121/150\n",
      "14/14 [==============================] - 2s 109ms/step - loss: 9.9796 - val_loss: 2.0164\n",
      "Epoch 122/150\n",
      "14/14 [==============================] - 2s 111ms/step - loss: 8.8549 - val_loss: 2.0167\n",
      "Epoch 123/150\n",
      "14/14 [==============================] - 2s 110ms/step - loss: 7.5675 - val_loss: 2.0170\n",
      "Epoch 124/150\n",
      "14/14 [==============================] - 2s 109ms/step - loss: 7.8975 - val_loss: 2.0174\n",
      "Epoch 125/150\n",
      "14/14 [==============================] - 1s 108ms/step - loss: 7.9065 - val_loss: 2.0178\n",
      "Epoch 126/150\n",
      "14/14 [==============================] - 2s 124ms/step - loss: 7.0708 - val_loss: 2.0180\n",
      "Epoch 127/150\n",
      "14/14 [==============================] - 2s 116ms/step - loss: 8.0227 - val_loss: 2.0184\n",
      "Epoch 128/150\n",
      "14/14 [==============================] - 2s 114ms/step - loss: 8.8074 - val_loss: 2.0187\n",
      "Epoch 129/150\n",
      "14/14 [==============================] - 2s 111ms/step - loss: 7.3655 - val_loss: 2.0191\n",
      "Epoch 130/150\n",
      "14/14 [==============================] - 2s 109ms/step - loss: 8.7043 - val_loss: 2.0194\n",
      "Epoch 131/150\n",
      "14/14 [==============================] - 2s 109ms/step - loss: 7.5339 - val_loss: 2.0198\n",
      "Epoch 132/150\n",
      "14/14 [==============================] - 2s 110ms/step - loss: 7.2682 - val_loss: 2.0201\n",
      "Epoch 133/150\n",
      "14/14 [==============================] - 2s 119ms/step - loss: 7.9986 - val_loss: 2.0204\n",
      "Epoch 134/150\n",
      "14/14 [==============================] - 2s 119ms/step - loss: 7.0272 - val_loss: 2.0208\n",
      "Epoch 135/150\n",
      "14/14 [==============================] - 1s 108ms/step - loss: 8.5692 - val_loss: 2.0211\n",
      "Epoch 136/150\n",
      "14/14 [==============================] - 2s 110ms/step - loss: 6.5800 - val_loss: 2.0214\n",
      "Epoch 137/150\n",
      "14/14 [==============================] - 2s 110ms/step - loss: 8.6105 - val_loss: 2.0217\n",
      "Epoch 138/150\n",
      "14/14 [==============================] - 2s 110ms/step - loss: 6.9588 - val_loss: 2.0221\n",
      "Epoch 139/150\n",
      "14/14 [==============================] - 2s 113ms/step - loss: 6.9801 - val_loss: 2.0224\n",
      "Epoch 140/150\n",
      "14/14 [==============================] - 2s 108ms/step - loss: 10.8585 - val_loss: 2.0227\n",
      "Epoch 141/150\n",
      "14/14 [==============================] - 2s 110ms/step - loss: 7.7249 - val_loss: 2.0231\n",
      "Epoch 142/150\n",
      "14/14 [==============================] - 2s 108ms/step - loss: 10.3234 - val_loss: 2.0234\n",
      "Epoch 143/150\n",
      "14/14 [==============================] - 2s 121ms/step - loss: 6.8834 - val_loss: 2.0237\n",
      "Epoch 144/150\n",
      "14/14 [==============================] - 2s 116ms/step - loss: 6.8096 - val_loss: 2.0241\n",
      "Epoch 145/150\n",
      "14/14 [==============================] - 2s 122ms/step - loss: 7.3768 - val_loss: 2.0244\n",
      "Epoch 146/150\n",
      "14/14 [==============================] - 2s 113ms/step - loss: 7.4621 - val_loss: 2.0247\n",
      "Epoch 147/150\n",
      "14/14 [==============================] - 2s 108ms/step - loss: 7.0986 - val_loss: 2.0250\n",
      "Epoch 148/150\n",
      "14/14 [==============================] - 2s 110ms/step - loss: 5.8636 - val_loss: 2.0252\n",
      "Epoch 149/150\n",
      "14/14 [==============================] - 2s 123ms/step - loss: 7.9006 - val_loss: 2.0255\n",
      "Epoch 150/150\n",
      "14/14 [==============================] - 2s 109ms/step - loss: 7.7672 - val_loss: 2.0258\n"
     ]
    }
   ],
   "source": [
    "history_2 = model_2.fit(X_array, y_array, validation_split = 0.2, epochs=150, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_2.history['loss'])\n",
    "plt.plot(history_2.history['val_loss'])\n",
    "plt.title('Model Loss - SGD')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would like to apply projects projects projects projects projects projects projects projects projects projects projects projects projects projects projects projects projects projects projects projects projects projects projects projects projects\n"
     ]
    }
   ],
   "source": [
    "sample = \"I would like to apply\"\n",
    "sample = sample.lower()\n",
    "sample_split = word_tokenize(sample)\n",
    "sample_split.insert(0, 0)\n",
    "\n",
    "for i in range(25):\n",
    "    sample_split = sample_split[1:]\n",
    "\n",
    "    sample_ids = [word_to_id[word] for word in sample_split]\n",
    "\n",
    "    sample_array = tf.convert_to_tensor(sample_ids)\n",
    "    sample_array = [list(sample_array)]\n",
    "    sample_array = np.reshape(sample_array, (1, seq_len, 1))\n",
    "    prediction = model_2.predict(sample_array)\n",
    "\n",
    "\n",
    "    pred_index = prediction[0].argmax()\n",
    "    new_word = tokens_list[pred_index]\n",
    "    sample_split.append(new_word)\n",
    "    sample = sample + ' ' + new_word\n",
    "\n",
    "sample = sample.capitalize()\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of both of these models is English text, but it does not make sense. The first model (using ADAM optimizer) seems to be better as the loss of the training and test sets actually do see to approach convergence. The second model's loss never changes. Since both models have an underfitting problem, the results could probably be improved by more training data. Neural nets require large training sets, and my corpus only contains 51 cover letter samples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
