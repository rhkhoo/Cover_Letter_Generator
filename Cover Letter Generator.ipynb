{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow\n",
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I will define a function to checkout some of the corpus statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_stats(corpus):\n",
    "  print('Corpus Stats:')\n",
    "  print('Number of Documents: ' + str(len(corpus.fileids())))\n",
    "  print('Number of Paragraphs ' + str(len(corpus.paras())))\n",
    "  print('Number of sentences: ' + str(len(corpus.sents())))\n",
    "  print('Number of words: ' + str(len(corpus.words())))\n",
    "  print(\"Vocabulary: \" + str(len(set(w.lower() for w in corpus.words()))))\n",
    "  print(\"Avg chars per word: \" + str(round(len(corpus.raw())/len(corpus.words()),1)))\n",
    "  print(\"Avg words per sentence: \" + str(round(len(corpus.words())/len(corpus.sents()),1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the sample cover letter .txt files via NLTK's PlaintextCorpusReader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './cover_letter_samples'\n",
    "doc_pattern = r'.*\\.txt'\n",
    "corpus = PlaintextCorpusReader(path, doc_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus Stats:\n",
      "Number of Documents: 51\n",
      "Number of Paragraphs 246\n",
      "Number of sentences: 625\n",
      "Number of words: 14942\n",
      "Vocabulary: 2564\n",
      "Avg chars per word: 5.6\n",
      "Avg words per sentence: 23.9\n"
     ]
    }
   ],
   "source": [
    "corpus_stats(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [corpus.raw(fileid) for fileid in corpus.fileids()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove \"\\n\" and \"\\r\" characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [doc.replace('\\n', '') for doc in docs]\n",
    "docs = [doc.replace('\\r', '') for doc in docs]\n",
    "docs = [doc.replace(')', '') for doc in docs]\n",
    "docs = [doc.replace('(', '') for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of individual words for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [word_tokenize(doc) for doc in docs]\n",
    "\n",
    "lens = [len(token) for token in tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a single list of all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list = []\n",
    "for token in tokenized:\n",
    "    tokens_list.extend(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will encode the text to numeric vectors using BERT encoder because it is pre-trained and can understand the meaning of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tz = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tz.encode_plus(\n",
    "    text=tokens_list,  # the text to be encoded\n",
    "    add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "    max_length = len(tokens_list),  # maximum length of a document\n",
    "    truncation = True,\n",
    "    padding = 'max_length',  # Add [PAD]s\n",
    "    return_attention_mask = True,  # Generate the attention mask\n",
    "    return_tensors = 'tf',  # ask the function to return TensorFlow tensors\n",
    ")\n",
    "input_ids = encoded['input_ids']\n",
    "attn_mask = encoded['attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BERT encoder outputs a list of lists, so I will consolidate them into a single list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_list = []\n",
    "for input_id in input_ids:\n",
    "    input_ids_list.extend(input_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BERT encoder also outputs tensors, I need to convert them to numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_int = []\n",
    "\n",
    "for tensor in input_ids_list:\n",
    "    input_ids_int.append(tensor.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the words are numeric vectors, I will need to be able to decode the model's output. I will also need to be able to encode a test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word = {input_ids_int[i]: tokens_list[i] for i in range(len(input_ids_int))}\n",
    "word_to_id = {tokens_list[i]: input_ids_int[i] for i in range(len(tokens_list))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are modeling sequence-to-sequence, I will create sequences and the word that immediately follows that sequence to use as \"labels.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for i in range(0, len(input_ids_list) - seq_len, 1):\n",
    "    in_seq = input_ids_list[i:i+seq_len]\n",
    "    out_seq = input_ids_list[i + seq_len]\n",
    "    X.append(in_seq)\n",
    "    y.append(out_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape the input to (*time steps*, *batch size*, *something else*) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_array = np.reshape(X, (len(X), seq_len, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_array = np_utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14084, 27934)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X_array.shape[1], X_array.shape[2]), return_sequences=True))\n",
    "model.add(Dense(256))\n",
    "model.add(LSTM(128, return_sequences = True))\n",
    "model.add(Dense(128))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dense(y_array.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm using ModelCheckpoint to save the model weights, so I don't have to retrain the model every time I restart the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"model_weights_saved.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "desired_callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "45/45 [==============================] - 31s 578ms/step - loss: 5.9268 - val_loss: 6.2361\n",
      "\n",
      "Epoch 00001: loss did not improve from 5.78636\n",
      "Epoch 2/20\n",
      "45/45 [==============================] - 28s 617ms/step - loss: 5.7495 - val_loss: 6.3832\n",
      "\n",
      "Epoch 00002: loss improved from 5.78636 to 5.75127, saving model to model_weights_saved.hdf5\n",
      "Epoch 3/20\n",
      "45/45 [==============================] - 26s 576ms/step - loss: 5.7007 - val_loss: 6.4334\n",
      "\n",
      "Epoch 00003: loss improved from 5.75127 to 5.72917, saving model to model_weights_saved.hdf5\n",
      "Epoch 4/20\n",
      "45/45 [==============================] - 30s 681ms/step - loss: 5.6825 - val_loss: 6.4399\n",
      "\n",
      "Epoch 00004: loss did not improve from 5.72917\n",
      "Epoch 5/20\n",
      "45/45 [==============================] - 28s 636ms/step - loss: 5.7305 - val_loss: 6.4684\n",
      "\n",
      "Epoch 00005: loss improved from 5.72917 to 5.72456, saving model to model_weights_saved.hdf5\n",
      "Epoch 6/20\n",
      "45/45 [==============================] - 29s 647ms/step - loss: 5.6990 - val_loss: 6.5051\n",
      "\n",
      "Epoch 00006: loss did not improve from 5.72456\n",
      "Epoch 7/20\n",
      "45/45 [==============================] - 31s 677ms/step - loss: 5.6709 - val_loss: 6.4715\n",
      "\n",
      "Epoch 00007: loss improved from 5.72456 to 5.71908, saving model to model_weights_saved.hdf5\n",
      "Epoch 8/20\n",
      "45/45 [==============================] - 31s 699ms/step - loss: 5.6762 - val_loss: 6.4839\n",
      "\n",
      "Epoch 00008: loss improved from 5.71908 to 5.71825, saving model to model_weights_saved.hdf5\n",
      "Epoch 9/20\n",
      "41/45 [==========================>...] - ETA: 2s - loss: 5.6600"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_array, y_array, validation_split = 0.2, epochs=20, batch_size=256, callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"model_weights_saved.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to test the model. I input the beginning of a cover letter. The loop deletes the first token in the sample, predicts on a sequence of the specified length, append the predicted word to the end of the sample. The first word is dropped and the model predicts on the next sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'excited', 'to', 'be', 'applying', 'for', 'your', 'company', 'because', 'I']\n",
      "[119, 100, 1821, 100, 1106, 1196, 1128, 1111, 1240, 1700, 119]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 11 into shape (1,10,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---\u001b[0m",
      "\u001b[1;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-4c2c4759ab5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0msample_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0msample_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0msample_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mreshape\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[1;34m(a, newshape, order)\u001b[0m\n\u001b[0;32m    297\u001b[0m            [5, 6]])\n\u001b[0;32m    298\u001b[0m     \"\"\"\n\u001b[1;32m--> 299\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'reshape'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mbound\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 11 into shape (1,10,1)"
     ]
    }
   ],
   "source": [
    "sample = \"I am excited to be applying for your company because I\"\n",
    "sample_split = word_tokenize(sample)\n",
    "sample_split.insert(0, 0)\n",
    "\n",
    "for i in range(5):\n",
    "    sample_split = sample_split[1:]\n",
    "    print(sample_split)\n",
    "    sample_ids = [word_to_id[word] for word in sample_split]\n",
    "    print(sample_ids)\n",
    "    sample_array = tf.convert_to_tensor(sample_ids)\n",
    "    sample_array = [list(sample_array)]\n",
    "    sample_array = np.reshape(sample_array, (1, seq_len, 1))\n",
    "    prediction = model.predict(sample_array)\n",
    "    print(prediction)\n",
    "    # the below line is not correct. For some reason only predicts 'analyses'\n",
    "    #pred_index = np.where(prediction == prediction.max())\n",
    "    #print(pred_index)\n",
    "    #ind_tup = list(zip(pred_index[0], pred_index[1]))[0]\n",
    "    #print(ind_tup)\n",
    "   \n",
    "    #new_word = tokens_list[pred_index]\n",
    "    #print(new_word)\n",
    "    #sample_split.append(new_word)\n",
    "    #sample = sample + ' ' + new_word\n",
    "\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is English text, but it does not make sense. The model simply predicts the word that appears immediately before it. This could probably be solved by more training data. Neural nets require large training sets, and my corpus only contains 51 cover letter samples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
