{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow\n",
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_stats(corpus):\n",
    "  print('Corpus Stats:')\n",
    "  print('Number of Documents: ' + str(len(corpus.fileids())))\n",
    "  print('Number of Paragraphs ' + str(len(corpus.paras())))\n",
    "  print('Number of sentences: ' + str(len(corpus.sents())))\n",
    "  print('Number of words: ' + str(len(corpus.words())))\n",
    "  print(\"Vocabulary: \" + str(len(set(w.lower() for w in corpus.words()))))\n",
    "  print(\"Avg chars per word: \" + str(round(len(corpus.raw())/len(corpus.words()),1)))\n",
    "  print(\"Avg words per sentence: \" + str(round(len(corpus.words())/len(corpus.sents()),1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './cover_letter_samples'\n",
    "doc_pattern = r'.*\\.txt'\n",
    "corpus = PlaintextCorpusReader(path, doc_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus Stats:\n",
      "Number of Documents: 32\n",
      "Number of Paragraphs 169\n",
      "Number of sentences: 429\n",
      "Number of words: 11042\n",
      "Vocabulary: 2298\n",
      "Avg chars per word: 5.5\n",
      "Avg words per sentence: 25.7\n"
     ]
    }
   ],
   "source": [
    "corpus_stats(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I would like to introduce myself as an applicant for the Data Scientist position at River Tech, a prestigious and reputable name in innovative technology. I am confident in my ability to perform as a Data Scientist at River Tech due to my extensive education and work experience.\\r\\n\\r\\nDuring my work experience at Crane & Jenkins, I had an extensive range of responsibilities including selecting features, optimizing classifiers, mining data, expanding the company's data by incorporating third-party sources, improving data collection techniques, processing data, and doing ad-hoc analyses. As a Data Scientist, I was required to have excellent communication skills, understanding of algorithms, excellence in the MatLab tool kit, proficiency in GGplot, knowledge of SQL, and excellence in applied statistics. During my eight-year tenure at Crane & Jenkins, I applied these skills daily and performed exceptionally at the company.\\r\\n\\r\\nMy abilities as a Data Scientist are rooted in a sturdy education in mathematics. I began with a bachelor's degree in computer science from Longford Tech. I followed this with a master's degree in statistics and a Ph.D. in applied mathematics. I attribute my success as a Data Scientist in large part to this extensive and in-depth education. I believe my personality has also played a major role in my ability to succeed in this career. I am an extremely analytical, data-oriented, and calculated. Even in my personal life I like to look at the data before making a decision. I like to analyze outcomes.\\r\\n\\r\\nI would like to thank you for taking the time to review my application. I look forward to hearing more about River Tech and the details of the Data Scientist position. I feel that my education and experience will ensure my success in this role.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [corpus.raw(fileid) for fileid in corpus.fileids()]\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [doc.replace('\\r\\n\\r\\n', '') for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I would like to introduce myself as an applicant for the Data Scientist position at River Tech, a prestigious and reputable name in innovative technology. I am confident in my ability to perform as a Data Scientist at River Tech due to my extensive education and work experience.During my work experience at Crane & Jenkins, I had an extensive range of responsibilities including selecting features, optimizing classifiers, mining data, expanding the company's data by incorporating third-party sources, improving data collection techniques, processing data, and doing ad-hoc analyses. As a Data Scientist, I was required to have excellent communication skills, understanding of algorithms, excellence in the MatLab tool kit, proficiency in GGplot, knowledge of SQL, and excellence in applied statistics. During my eight-year tenure at Crane & Jenkins, I applied these skills daily and performed exceptionally at the company.My abilities as a Data Scientist are rooted in a sturdy education in mathematics. I began with a bachelor's degree in computer science from Longford Tech. I followed this with a master's degree in statistics and a Ph.D. in applied mathematics. I attribute my success as a Data Scientist in large part to this extensive and in-depth education. I believe my personality has also played a major role in my ability to succeed in this career. I am an extremely analytical, data-oriented, and calculated. Even in my personal life I like to look at the data before making a decision. I like to analyze outcomes.I would like to thank you for taking the time to review my application. I look forward to hearing more about River Tech and the details of the Data Scientist position. I feel that my education and experience will ensure my success in this role.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [word_tokenize(doc) for doc in docs]\n",
    "\n",
    "lens = [len(token) for token in tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list = []\n",
    "for token in tokenized:\n",
    "    tokens_list.extend(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tz = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tz.encode_plus(\n",
    "    text=tokens_list,  # the text to be encoded\n",
    "    add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "    max_length = len(tokens_list),  # maximum length of a document\n",
    "    truncation = True,\n",
    "    padding = 'max_length',  # Add [PAD]s\n",
    "    return_attention_mask = True,  # Generate the attention mask\n",
    "    return_tensors = 'tf',  # ask the function to return TensorFlow tensors\n",
    ")\n",
    "input_ids = encoded['input_ids']\n",
    "attn_mask = encoded['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_list = []\n",
    "for input_id in input_ids:\n",
    "    input_ids_list.extend(input_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_int = []\n",
    "\n",
    "for tensor in input_ids_list:\n",
    "    input_ids_int.append(tensor.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For use decoding output\n",
    "id_to_word = {input_ids_int[i]: tokens_list[i] for i in range(len(input_ids_int))}\n",
    "word_to_id = {tokens_list[i]: input_ids_int[i] for i in range(len(tokens_list))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for i in range(0, len(input_ids_list) - 100, 1):\n",
    "    in_seq = input_ids_list[i:i+100]\n",
    "    out_seq = input_ids_list[i + 100]\n",
    "    X.append(in_seq)\n",
    "    y.append(out_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_array = np.reshape(X, (len(X), 100, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_array[0]\n",
    "X_train = X_array[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10458, 100, 1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.reshape(X_val, (1, 100, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100, 1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_array = np_utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_array[1:]\n",
    "y_test = y_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X_array.shape[1], X_array.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_array.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"model_weights_saved.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "desired_callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "41/41 [==============================] - 172s 4s/step - loss: 9.0838\n",
      "\n",
      "Epoch 00001: loss improved from inf to 7.89874, saving model to model_weights_saved.hdf5\n",
      "Epoch 2/4\n",
      "41/41 [==============================] - 162s 4s/step - loss: 6.0040\n",
      "\n",
      "Epoch 00002: loss improved from 7.89874 to 6.02796, saving model to model_weights_saved.hdf5\n",
      "Epoch 3/4\n",
      "41/41 [==============================] - 185s 5s/step - loss: 5.9231\n",
      "\n",
      "Epoch 00003: loss improved from 6.02796 to 5.94107, saving model to model_weights_saved.hdf5\n",
      "Epoch 4/4\n",
      "41/41 [==============================] - 163s 4s/step - loss: 5.8445\n",
      "\n",
      "Epoch 00004: loss improved from 5.94107 to 5.90234, saving model to model_weights_saved.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20075b67b48>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, validation_split = 0.2, epochs=4, batch_size=256, callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"model_weights_saved.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict(X_test)\n",
    "prediction[0].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_list[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ample = 'I am applying for the position of <job>'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
